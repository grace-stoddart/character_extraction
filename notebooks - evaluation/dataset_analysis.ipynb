{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This notebook contains rough code to analyse the makeup of different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from SVM_functions import param_selection, train_and_evaluate_model, combine_features\n",
    "from misc import save_dict, get_file_names, open_dict\n",
    "from eval_functions import get_ref_exps_from_coref_dict\n",
    "from settings import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get num coref chains & num labelled 'character' in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProppLearner_from_gold\n",
      "num coref chains: 1633\n",
      "num characters: 123\n",
      "frac characters: 0.08\n",
      "\n",
      "ProppLearner_from_allen\n",
      "num coref chains: 2266\n",
      "num characters: 702\n",
      "frac characters: 0.31\n",
      "\n",
      "ProppLearner_from_heads_only\n",
      "num coref chains: 1912\n",
      "num characters: 564\n",
      "frac characters: 0.29\n",
      "\n",
      "LitBank_from_gold\n",
      "num coref chains: 2849\n",
      "num characters: 66\n",
      "frac characters: 0.02\n",
      "\n",
      "LitBank_from_allen\n",
      "num coref chains: 1348\n",
      "num characters: 48\n",
      "frac characters: 0.04\n",
      "\n",
      "CEN_from_allen\n",
      "num coref chains: 1900\n",
      "num characters: 185\n",
      "frac characters: 0.1\n",
      "\n",
      "CEN_from_heads_only\n",
      "num coref chains: 17251\n",
      "num characters: 436\n",
      "frac characters: 0.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for expName, expSettings in settings.items():\n",
    "\n",
    "    featuresDir = expSettings['featuresDir']\n",
    "    fileNames = get_file_names(expSettings['corefDir'])\n",
    "    y = combine_features(featuresDir, [expSettings['character labels dir extention']], fileNames).transpose()\n",
    "\n",
    "    print(expName)\n",
    "    print('num coref chains:', int(y.shape[0]))\n",
    "    print('num characters:', int(np.sum(y)))\n",
    "    print('frac characters:', round( int(np.sum(y)) / int(y.shape[0]) , 2))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get num tokens in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "46\n",
      "46\n",
      "38\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "datasets = {'CEN':'tokenized', 'ProppLearner':'tokenized', 'LitBank':'tokenized_shortened'}\n",
    "\n",
    "lens = []\n",
    "\n",
    "for dataset, folderName in datasets.items():\n",
    "    lengths = []\n",
    "    tokensDir = \"../data/\" + dataset + '/' + folderName + '/'\n",
    "\n",
    "    fileNames = get_file_names(tokensDir, '.p')\n",
    "\n",
    "\n",
    "    for fileName in fileNames:\n",
    "        tokenized = open_dict(tokensDir + fileName + '.p')\n",
    "\n",
    "        lengths.append(len(tokenized['tokens']))\n",
    "\n",
    "    print(len(fileNames))\n",
    "    print(len(lengths))\n",
    "    lens.append(lengths)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4414.066666666667\n",
      "2370.021739130435\n",
      "2076.684210526316\n"
     ]
    }
   ],
   "source": [
    "for a in lens:\n",
    "    print(sum(a)/len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greater 7\n",
      "less 107\n"
     ]
    }
   ],
   "source": [
    "x = 6300\n",
    "\n",
    "gt = 0\n",
    "lt = 0\n",
    "for lengths in lens:\n",
    "    for l in lengths:\n",
    "        if l >= x:\n",
    "            gt += 1\n",
    "        else:\n",
    "            lt += 1\n",
    "\n",
    "print('greater',gt)\n",
    "print('less',lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(lens[0]) / len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11560\\4196196295.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "print(len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "### ProppLEarner Gold\n",
    "datasets = {'ProppLearner':'tokenized'}\n",
    "\n",
    "lens = []\n",
    "\n",
    "for dataset, folderName in datasets.items():\n",
    "    lengths = []\n",
    "    tokensDir = \"../data/\" + dataset + '/' + folderName + '/'\n",
    "\n",
    "    fileNames = get_file_names(tokensDir, '.p')\n",
    "\n",
    "\n",
    "    for fileName in fileNames:\n",
    "        if int(fileName[5:]) not in list(range(1,16)):\n",
    "            continue\n",
    "        tokenized = open_dict(tokensDir + fileName + '.p')\n",
    "\n",
    "        lengths.append(len(tokenized['tokens']))\n",
    "\n",
    "    print(len(fileNames))\n",
    "    print(len(lengths))\n",
    "    lens.append(lengths)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[754,\n",
       " 932,\n",
       " 1499,\n",
       " 1591,\n",
       " 2187,\n",
       " 2169,\n",
       " 2288,\n",
       " 866,\n",
       " 1121,\n",
       " 1227,\n",
       " 1522,\n",
       " 1501,\n",
       " 2128,\n",
       " 1679,\n",
       " 1818]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### of animate, how many are character and how many are not character? what's the brakdown for inanimate? (propplearner gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresDir = '../intermediate/ProppLearner/from_gold_corefs/'\n",
    "fileNames = get_file_names(featuresDir + 'animacy_labels_gold/', '.npy')\n",
    "\n",
    "\n",
    "animLabels = combine_features(featuresDir, ['animacy_labels_gold'], fileNames)\n",
    "charLabels = combine_features(featuresDir, ['character_labels_gold'], fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "animChar = 0\n",
    "animNotChar = 0\n",
    "\n",
    "notAnimChar = 0\n",
    "notAnimNotChar = 0\n",
    "\n",
    "for animLabel, charLabel in zip(animLabels[0], charLabels[0]):\n",
    "\n",
    "    if animLabel == 1.:\n",
    "        if charLabel == 1.:\n",
    "            animChar += 1\n",
    "        else:\n",
    "            animNotChar += 1\n",
    "\n",
    "    else:\n",
    "        if charLabel == 1.:\n",
    "            notAnimChar += 1\n",
    "        else:\n",
    "            notAnimNotChar += 1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 207 330\n",
      "0 1303\n"
     ]
    }
   ],
   "source": [
    "print(animChar, animNotChar, animChar+animNotChar)\n",
    "print(notAnimChar, notAnimNotChar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of alen coref chains, qualititative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corefs gold new format\n",
    "\n",
    "vs\n",
    "\n",
    "corefs allennlp (first 15 stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "corefsDir = ['../data/ProppLearner/corefs_gold_new_format/','../data/ProppLearner/corefs_allen/']\n",
    "\n",
    "featuresDir = ['../intermediate/ProppLearner/from_gold_corefs/character_labels_gold/','../intermediate/ProppLearner/from_allenNLP_corefs/character_labels_scraped/']\n",
    "\n",
    "fileNames = get_file_names(corefsDir[0], '.p')\n",
    "\n",
    "HRDir  = ['../data/ProppLearner/HR_corefs_gold_new_format/', '../data/ProppLearner/HR_corefs_allen/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileName in fileNames:\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        corefs = open_dict(corefsDir[i] + fileName + '.p')\n",
    "        charLabels = np.load(featuresDir[i] + fileName + '.npy')\n",
    "\n",
    "        \n",
    "\n",
    "        refs = get_ref_exps_from_coref_dict(corefs)\n",
    "\n",
    "        stringToSave = ''\n",
    "        for refNum, ref in enumerate(refs):\n",
    "            stringToSave += str(charLabels[refNum])\n",
    "\n",
    "            for r in ref:\n",
    "                stringToSave += ' | ' + str(r)\n",
    "\n",
    "            stringToSave += '\\n'\n",
    "            \n",
    "        text_file = open( HRDir[i] + fileName + '.txt', \"w\")\n",
    "        n = text_file.write(stringToSave)\n",
    "        text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in [1]:\n",
    "\n",
    "        fileName = 'story21'\n",
    "\n",
    "        corefs = open_dict(corefsDir[i] + fileName + '.p')\n",
    "        charLabels = np.load(featuresDir[i] + fileName + '.npy')\n",
    "\n",
    "        \n",
    "\n",
    "        refs = get_ref_exps_from_coref_dict(corefs)\n",
    "\n",
    "        stringToSave = ''\n",
    "        for refNum, ref in enumerate(refs):\n",
    "            stringToSave += str(charLabels[refNum])\n",
    "\n",
    "            for r in ref:\n",
    "                stringToSave += ' | ' + str(r)\n",
    "\n",
    "            stringToSave += '\\n'\n",
    "            \n",
    "        text_file = open( HRDir[i] + fileName + '.txt', \"w\")\n",
    "        n = text_file.write(stringToSave)\n",
    "        text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('character')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b542c7ce8dff2676b97f07a4c5c5770771126361134ac0530a2bffe8df31a89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
