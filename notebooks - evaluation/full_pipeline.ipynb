{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This Notebook runs experiments for the full char extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from SVM_functions import param_selection, train_and_evaluate_model, combine_features\n",
    "from misc import save_dict, get_file_names, open_dict, open_list\n",
    "from eval_functions import combine_corefs, get_ref_exps_from_coref_dict, get_all_variations, score_output, process_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results - 5 fold cross validation, repeated 20 times.\n",
    "F-Score claculated based on outputted character list & ground truth characted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../intermediate/ProppLearner/from_gold_corefs/CL/story32.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16899/2723843841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# get training features & labels (over and under sampling)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mfeaturesTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesDirTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureNamesTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileNamesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturesTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturesTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_MSc/character_identification_working/src/SVM_functions.py\u001b[0m in \u001b[0;36mcombine_features\u001b[0;34m(featuresDir, featureNames, fileNames)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileNames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mfeaturesStory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeatureNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/character-allennlp-4/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../intermediate/ProppLearner/from_gold_corefs/CL/story32.npy'"
     ]
    }
   ],
   "source": [
    "useGoldForTraining = True\n",
    "\n",
    "for tryNum in range(5):\n",
    "\n",
    "    settings = open_dict('../results/ablation_testing/settings.p')\n",
    "\n",
    "    settingsFromAllen = {\n",
    "            'ProppLearner': {\n",
    "                'normal':settings['ProppLearner_from_allen'],\n",
    "                'goldStandard':settings['ProppLearner_from_gold']\n",
    "            },\n",
    "            # 'CEN': {\n",
    "            #     'normal':settings['CEN_from_allen'],\n",
    "            # },\n",
    "            'LitBank': {\n",
    "                'normal':settings['LitBank_from_allen'],\n",
    "                'goldStandard':settings['LitBank_from_gold']\n",
    "            },\n",
    "        }\n",
    "\n",
    "    numFolds = 5\n",
    "    numRepeats = 20\n",
    "    numSettings = len(settingsFromAllen)\n",
    "\n",
    "    results = np.zeros((numSettings, 4, 4, numFolds, numRepeats))\n",
    "    # test = np.object0((numSettings, numFolds, numRepeats))\n",
    "\n",
    "    for setNum, (settingName, setting) in enumerate(settingsFromAllen.items()):\n",
    "        \n",
    "        # get directories, parameters & file names for the dataset from settings\n",
    "\n",
    "        if useGoldForTraining:\n",
    "            trainSetting = 'goldStandard'\n",
    "        else:\n",
    "            trainSetting = 'normal'\n",
    "            \n",
    "\n",
    "        featuresDirTest = setting['normal']['featuresDir']\n",
    "        featuresDirTrain = setting[trainSetting]['featuresDir']\n",
    "        corefsDir = setting['normal']['corefDir']\n",
    "        fileNames = get_file_names(corefsDir, '.p')\n",
    "        params = setting['normal']['parameters']['additional features']\n",
    "\n",
    "        # get list of features to use\n",
    "        featureNamesTest = ['CL', 'DP', 'NE', 'SS', 'TP', 'WN', 'CD', 'QU', 'CP']\n",
    "        featureNamesTest.append(setting['normal']['animacy labels dir extention'])\n",
    "        featureNamesTest.append(setting['normal']['character labels dir extention']) \n",
    "\n",
    "        featureNamesTrain = ['CL', 'DP', 'NE', 'SS', 'TP', 'WN', 'CD', 'QU', 'CP']\n",
    "        featureNamesTrain.append(setting[trainSetting]['animacy labels dir extention'])\n",
    "        featureNamesTrain.append(setting[trainSetting]['character labels dir extention']) \n",
    "\n",
    "        # turn list of file names into an array and shuffle\n",
    "        fileNames = np.array(fileNames)\n",
    "        fileNames = np.random.permutation(fileNames)\n",
    "\n",
    "        # set up k fold cross validation\n",
    "        kf = KFold(n_splits = numFolds)\n",
    "\n",
    "        for repeatNum in range(numRepeats):\n",
    "\n",
    "            for foldNum, (train_idx, test_idx) in enumerate(kf.split(fileNames)):\n",
    "\n",
    "                # get file names for test, and file names for training\n",
    "                fileNamesTrain = fileNames[train_idx]\n",
    "                fileNamesTest = fileNames[test_idx]\n",
    "\n",
    "                # get training features & labels (over and under sampling)\n",
    "                featuresTrain = combine_features(featuresDirTrain, featureNamesTrain, list(fileNamesTrain)).transpose()\n",
    "                X_train = featuresTrain[:, :-1]\n",
    "                y_train = featuresTrain[:, -1]\n",
    "\n",
    "                over = SMOTE(sampling_strategy = 0.8)\n",
    "                under = RandomUnderSampler(sampling_strategy = 1.0)\n",
    "                X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "                X_train, y_train = under.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "                # train model\n",
    "                model = svm.SVC(kernel = params['kernel'], C = params['C'], gamma = params['gamma'])\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                ### scoring works for CEN and ProppLearner\n",
    "                testResults = np.zeros((len(fileNamesTest), 4, 4))\n",
    "\n",
    "                for testFileNum, fileName in enumerate(fileNamesTest):\n",
    "                    \n",
    "                    # get features for test story, and get predications from model\n",
    "                    featuresTest = combine_features(featuresDirTest, featureNamesTest, [fileName]).transpose()\n",
    "                    X_test = featuresTest[:, :-1]\n",
    "                    _ = featuresTest[:, -1]\n",
    "\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                    # get output coref chains\n",
    "                    corefDict = open_dict(corefsDir + fileName + '.p')\n",
    "                    outputChains = get_ref_exps_from_coref_dict(corefDict)\n",
    "\n",
    "                    # post process y_pred\n",
    "                    ### get rid of 'None'\n",
    "                    y_pred_none = y_pred.copy()\n",
    "                    for k, (pred, corefChain) in enumerate(zip(y_pred_none, outputChains)):\n",
    "                    \n",
    "                        if corefChain[0] == None:\n",
    "                            y_pred_none[k] = 0\n",
    "\n",
    "                        elif corefChain[0] == \"'s\":\n",
    "                            y_pred_none[k] = 0\n",
    "\n",
    "                    ### get rid of 'and's\n",
    "                    y_pred_and = y_pred.copy()\n",
    "                    for k, (pred, corefChain) in enumerate(zip(y_pred_and, outputChains)):\n",
    "                    \n",
    "                        if corefChain[0] != None and ' and ' in corefChain[0]:\n",
    "                            y_pred_and[k] = 0\n",
    "\n",
    "                    ### combine both\n",
    "                    y_pred_both = y_pred_and * y_pred_none\n",
    "\n",
    "                    # get ground truth character coref chains\n",
    "\n",
    "                    \n",
    "                    if settingName == 'ProppLearner':\n",
    "                        fileNameChars = fileName[0].upper() + fileName[1:]\n",
    "                    else:\n",
    "                        fileNameChars = fileName\n",
    "\n",
    "                    ####### PropLearner and CEN\n",
    "                    if settingName in ['ProppLearner', 'CEN']:\n",
    "                        charChains = open_list('../data/'+settingName+'/char_list_gold_full_chains/'+fileNameChars+'.json')\n",
    "                    \n",
    "\n",
    "                    else:\n",
    "                        charNames = open_list('../data/LitBank/char_list_gold/' + fileNameChars + '.json')\n",
    "                        charChains = get_all_variations(charNames)\n",
    "\n",
    "                    # calculate prec, recall, accuracy and f1 for each version of y_pred\n",
    "                    # precision, recall, accuracy, f1\n",
    "                    testResults[testFileNum, 0, : ] = score_output(charChains, y_pred, outputChains)[:4]\n",
    "                    testResults[testFileNum, 1, : ] = score_output(charChains, y_pred_both, outputChains)[:4]\n",
    "                    testResults[testFileNum, 2, : ] = score_output(charChains, y_pred_none, outputChains)[:4]\n",
    "                    testResults[testFileNum, 3, : ] = score_output(charChains, y_pred_and, outputChains)[:4]\n",
    "\n",
    "                    # Put together human readable output (only saved for the first fold in k-fold cross validation) \n",
    "                    resultsFileText = settingName\n",
    "\n",
    "                    if repeatNum == 0:\n",
    "                        for fileName in fileNamesTest:\n",
    "                            featuresTest = combine_features(featuresDirTest, featureNamesTest, [fileName]).transpose()\n",
    "\n",
    "                            X_test = featuresTest[:, :-1]\n",
    "                            y_test = featuresTest[:, -1]\n",
    "                            \n",
    "                            y_pred = model.predict(X_test)\n",
    "\n",
    "                            corefDict = open_dict(corefsDir + fileName + '.p')\n",
    "                            outputChains = get_ref_exps_from_coref_dict(corefDict)\n",
    "\n",
    "                            # get y_pred_both\n",
    "                            y_pred_none = y_pred.copy()\n",
    "                            for k, (pred, corefChain) in enumerate(zip(y_pred_none, outputChains)):\n",
    "                            \n",
    "                                if corefChain[0] == None:\n",
    "                                    y_pred_none[k] = 0\n",
    "\n",
    "                                elif corefChain[0] == \"'s\":\n",
    "                                    y_pred_none[k] = 0\n",
    "\n",
    "                            y_pred_and = y_pred.copy()\n",
    "                            for k, (pred, corefChain) in enumerate(zip(y_pred_and, outputChains)):\n",
    "                            \n",
    "                                if corefChain[0] != None and ' and ' in corefChain[0]:\n",
    "                                    y_pred_and[k] = 0\n",
    "\n",
    "                            y_pred_both = y_pred_and * y_pred_none\n",
    "\n",
    "                            #\n",
    "                            if settingName == 'ProppLearner':\n",
    "                                fileNameChars = fileName[0].upper() + fileName[1:]\n",
    "                            else:\n",
    "                                fileNameChars = fileName\n",
    "\n",
    "                            resultsFileText += '\\n\\n' + fileName + '\\n'\n",
    "\n",
    "                            # orig Jahan character coref chains\n",
    "                            if not 'LitBank' in settingName:\n",
    "                                charChains = open_list('../data/'+settingName+'/char_list_gold_full_chains/'+fileNameChars+'.json')\n",
    "                                charLabels, charVotes = score_output(charChains, y_pred_both, outputChains)[-2:]\n",
    "                                \n",
    "                                for chainNum, chain in enumerate(charChains):\n",
    "                                    resultsFileText += str(charVotes[chainNum]) + ' | ' +  ' | '.join(chain) + '\\n'\n",
    "                                \n",
    "                            else:\n",
    "                                charNames = open_list('../data/LitBank/char_list_gold/' + fileNameChars + '.json')\n",
    "                                charChains = get_all_variations(charNames)\n",
    "                                charLabels, charVotes = score_output(charChains, y_pred_both, outputChains)[-2:]\n",
    "                                for chainNum, chain in enumerate(charChains):\n",
    "                                    resultsFileText += str(charVotes[chainNum]) + ' | ' + ' | '.join(chain) + '\\n'\n",
    "                            \n",
    "\n",
    "                            \n",
    "                            for i, chain in enumerate(outputChains):\n",
    "                                if y_pred_both[i] == 1.:\n",
    "                                    # if chain[0] == None:\n",
    "                                    #     continue\n",
    "                                    refsPrint = str(charLabels[i]) + ' | ' +  str(chain[0]) + ' |'\n",
    "                                    for j in range(1, len(chain)):\n",
    "                                        refsPrint += '| ' + chain[j] + ' ' \n",
    "\n",
    "                                    resultsFileText += '\\n' + str(y_pred_both[i]) + ' | ' + refsPrint            \n",
    "\n",
    "                        with open('../results/full_pipeline/' + settingName + str(foldNum) + '.txt', 'w') as f:\n",
    "                            f.write(resultsFileText)\n",
    "\n",
    "\n",
    "                testResults = np.mean(testResults, axis = 0)\n",
    "                results[setNum, :, :, foldNum, repeatNum] = testResults\n",
    "\n",
    "    results = np.mean(results, axis = 4)\n",
    "    results = np.mean(results, axis = 3)\n",
    "\n",
    "    np.save('../results/full_pipeline/results_gold_for_labelling' + str(tryNum), results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare results old char labelling vs new char labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ave_results(fileNameBase):\n",
    "    \n",
    "    results = np.load('../results/full_pipeline/' + fileNameBase + '0.npy')\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        results += np.load('../results/full_pipeline/' + fileNameBase + str(i) + '.npy')\n",
    "\n",
    "    return results / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new labelling [[[0.41242984 0.54006123 0.41242984 0.45390619]\n",
      "  [0.46155634 0.53483573 0.46155634 0.48155781]\n",
      "  [0.43661723 0.54006123 0.43661723 0.46890208]\n",
      "  [0.43504659 0.53483573 0.43504659 0.4657462 ]]\n",
      "\n",
      " [[0.41420114 0.38324808 0.41420114 0.37722727]\n",
      "  [0.44467015 0.37360103 0.44467015 0.38652053]\n",
      "  [0.41853352 0.38324808 0.41853352 0.37981575]\n",
      "  [0.43947051 0.37360103 0.43947051 0.38356567]]]\n",
      "\n",
      "old labelling [[[0.41377981 0.54160233 0.41377981 0.45512876]\n",
      "  [0.46269896 0.53665517 0.46269896 0.48259645]\n",
      "  [0.4377362  0.54160233 0.4377362  0.46994896]\n",
      "  [0.43617776 0.53665517 0.43617776 0.46692263]]\n",
      "\n",
      " [[0.4051202  0.38023847 0.4051202  0.37147997]\n",
      "  [0.43613201 0.37099011 0.43613201 0.38161959]\n",
      "  [0.40963106 0.38023847 0.40963106 0.37417137]\n",
      "  [0.43075687 0.37099011 0.43075687 0.37856104]]\n",
      "\n",
      " [[0.20250394 0.16658848 0.20250394 0.1516068 ]\n",
      "  [0.22873628 0.16658848 0.22873628 0.15922965]\n",
      "  [0.2114833  0.16658848 0.2114833  0.15282152]\n",
      "  [0.21974263 0.16658848 0.21974263 0.15799996]]]\n"
     ]
    }
   ],
   "source": [
    "print('new labelling', get_ave_results('results_new_char_labelling'))\n",
    "print()\n",
    "print('old labelling', get_ave_results('results'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('character')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b542c7ce8dff2676b97f07a4c5c5770771126361134ac0530a2bffe8df31a89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
