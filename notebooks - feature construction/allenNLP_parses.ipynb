{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from misc import open_dict, save_dict, get_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### ProppLearner from gold standard coreference chains & labels #####\n",
    "# tokensDir= \"../data/ProppLearner/tokenized/\"\n",
    "# parsesDir = '../intermediate/ProppLearner/parses/'\n",
    "# fileNames = get_file_names('data/ProppLearner/corefs_gold_new_format/', '.p')\n",
    "\n",
    "##### Propp Learner from allenNLP coreference chains & approximate labels #####\n",
    "tokensDir= \"../data/ProppLearner/tokenized/\"\n",
    "parsesDir = '../intermediate/ProppLearner/parses/'\n",
    "fileNames = get_file_names('../data/ProppLearner/corefs_allen/', '.p')\n",
    "\n",
    "# ##### LitBank from gold standard coreference chains #####\n",
    "# tokensDir = \"../data/LitBank/tokenized_shortened/\"\n",
    "# parsesDir = '../intermediate/LitBank/parses_shortened/'\n",
    "# fileNames = get_file_names('data/LitBank/corefs_gold_new_format/', '.p')\n",
    "\n",
    "# ##### LitBank from AllenNLP coreference chains #####\n",
    "# tokensDir = \"../data/LitBank/tokenized/\"\n",
    "# parsesDir = '../intermediate/LitBank/parses/'\n",
    "# fileNames = get_file_names('data/LitBank/corefs_allen/', '.p')\n",
    "\n",
    "\n",
    "depParseLoc = parsesDir + 'deps-biaffine/'\n",
    "nerParseLoc = parsesDir + 'NER-elmo/'\n",
    "openParseLoc = parsesDir + 'openIE/'\n",
    "srlParseLoc = parsesDir + 'SRL-bert/'\n",
    "\n",
    "constParseLoc = parsesDir + 'constituency/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorSRL = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
    "predictorDep = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\")\n",
    "predictorNER = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz\")\n",
    "predictorOpen = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, fileName in enumerate(fileNames):\n",
    "\n",
    "    document = open_dict(tokensDir + fileName + '.p')\n",
    "\n",
    "    # do semantic relation parse of each sentence in document\n",
    "    srlParses = []\n",
    "    for sent in document['sents']:\n",
    "        sTokens = sent['tokens']\n",
    "        srlParse = predictorSRL.predict_tokenized(tokenized_sentence = sTokens)\n",
    "        srlParses.append(srlParse)\n",
    "\n",
    "    srlParseDict = {'parses': srlParses, 'document':document}\n",
    "    save_dict(srlParseDict, srlParseLoc + fileName + '.p')\n",
    "\n",
    "    # do semantic relation parse of each sentence in document\n",
    "    depParses = []\n",
    "    for sent in document['sents']:\n",
    "        s = sent['text']\n",
    "        depParse = predictorDep.predict(sentence = s)\n",
    "        depParses.append(depParse)\n",
    "\n",
    "    depParseDict = {'parses': depParses, 'document':document}\n",
    "    save_dict(depParseDict, depParseLoc + fileName + '.p')\n",
    "\n",
    "    # do openIE parse of each sentence in document\n",
    "    openParses = []\n",
    "    for sent in document['sents']:\n",
    "        s = sent['text']\n",
    "        openParse = predictorOpen.predict(sentence = s)\n",
    "        openParses.append(openParse)\n",
    "\n",
    "    openParseDict = {'parses': openParses, 'document':document}\n",
    "    save_dict(openParseDict, openParseLoc + fileName + '.p')\n",
    "\n",
    "    # do semantic relation parse of each sentence in document\n",
    "    nerParses = []\n",
    "    for sent in document['sents']:\n",
    "        s = sent['text']\n",
    "        nerParse = predictorNER.predict(sentence = s)\n",
    "        nerParses.append(nerParse)\n",
    "\n",
    "    nerParseDict = {'parses': nerParses, 'document':document}\n",
    "    save_dict(nerParseDict, nerParseLoc + fileName + '.p')\n",
    "\n",
    "    print(num, fileName, 'done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Constituency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "predictorCP = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_163698/89214089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mconstParse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictorCP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mconstParses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstParse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/character-allennlp-4/lib/python3.7/site-packages/allennlp_models/structured_prediction/predictors/srl.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msemantic\u001b[0m \u001b[0mroles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_tokenized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/character-allennlp-4/lib/python3.7/site-packages/allennlp_models/structured_prediction/predictors/srl.py\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"verbs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"words\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/character-allennlp-4/lib/python3.7/site-packages/allennlp_models/structured_prediction/predictors/srl.py\u001b[0m in \u001b[0;36mpredict_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"verbs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"words\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/character-allennlp-4/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_output_human_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             instance_separated_output: List[Dict[str, numpy.ndarray]] = [\n",
      "\u001b[0;32m~/miniconda3/envs/character-allennlp-4/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/character-allennlp-4/lib/python3.7/site-packages/allennlp_models/structured_prediction/models/srl_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, verb_indicator, metadata, tags)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         )\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"class_probabilities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclass_probabilities\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;31m# We need to retain the mask in the output dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# so that we can crop the sequences to remove padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for num, fileName in enumerate(fileNames):\n",
    "\n",
    "    document = open_dict(tokensDir + fileName + '.p')\n",
    "\n",
    "    constParses = []\n",
    "    for sent in document['sents']:\n",
    "        s = sent['text']\n",
    "        constParse = predictorCP.predict(sentence = s)\n",
    "        constParses.append(constParse)\n",
    "\n",
    "    constParseDict = {'parses': constParses, 'document':document}\n",
    "    save_dict(constParseDict, constParseLoc + fileName + '.p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "constParse = open_dict('../intermediate/ProppLearner/parses/constituency/story1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = open_dict('../data/ProppLearner/tokenized/story1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arg0s(constParses, sentences):\n",
    "    positions = []\n",
    "\n",
    "    tokenOffset = 0\n",
    "\n",
    "    for i, parse in enumerate(constParses['parses']):\n",
    "        \n",
    "\n",
    "        for verb in parse['verbs']:\n",
    "            for j, tag in enumerate(verb['tags']):\n",
    "                if tag == 'B-ARG0':\n",
    "                    count = 0\n",
    "\n",
    "                    currIndex = j\n",
    "\n",
    "                    while currIndex < len(verb['tags']) -1:\n",
    "                        currIndex += 1\n",
    "                        if verb['tags'][currIndex] != 'I-ARG0':\n",
    "                            break\n",
    "                        count+= 1\n",
    "\n",
    "                    positions.append([j+tokenOffset, j+count+tokenOffset])\n",
    "\n",
    "        tokenOffset += len(sentences[i]['tokens'])\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arg1s(constParses, sentences):\n",
    "    positions = []\n",
    "\n",
    "    tokenOffset = 0\n",
    "\n",
    "    for i, parse in enumerate(constParses['parses']):\n",
    "        \n",
    "\n",
    "        for verb in parse['verbs']:\n",
    "            for j, tag in enumerate(verb['tags']):\n",
    "                if tag == 'B-ARG1':\n",
    "                    count = 0\n",
    "\n",
    "                    currIndex = j\n",
    "\n",
    "                    while currIndex < len(verb['tags']) -1:\n",
    "                        currIndex += 1\n",
    "                        if verb['tags'][currIndex] != 'I-ARG1':\n",
    "                            break\n",
    "                        count+= 1\n",
    "\n",
    "                    positions.append([j+tokenOffset, j+count+tokenOffset])\n",
    "\n",
    "        tokenOffset += len(sentences[i]['tokens'])\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arg2s(constParses, sentences):\n",
    "    positions = []\n",
    "\n",
    "    tokenOffset = 0\n",
    "\n",
    "    for i, parse in enumerate(constParses['parses']):\n",
    "        \n",
    "\n",
    "        for verb in parse['verbs']:\n",
    "            for j, tag in enumerate(verb['tags']):\n",
    "                if tag == 'B-ARG2':\n",
    "                    count = 0\n",
    "\n",
    "                    currIndex = j\n",
    "\n",
    "                    while currIndex < len(verb['tags']) -1:\n",
    "                        currIndex += 1\n",
    "                        if verb['tags'][currIndex] != 'I-ARG2':\n",
    "                            break\n",
    "                        count+= 1\n",
    "\n",
    "                    positions.append([j+tokenOffset, j+count+tokenOffset])\n",
    "\n",
    "        tokenOffset += len(sentences[i]['tokens'])\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg1s = get_arg1s(constParse, document['sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dragon \n",
      "heavy tribute \n",
      "a lovely maiden from every house \n",
      "to go to the dragon \n",
      "her \n",
      "her \n",
      "her \n",
      "she \n",
      "her \n",
      "his house \n",
      "the princess \n",
      "a little dog that had followed her to the dragon 's lair \n",
      "her letter \n",
      "it \n",
      "the answer \n",
      "to find out who in this world was stronger than the dragon \n",
      "who in this world was stronger than the dragon \n",
      "who in this world \n",
      "The princess \n",
      "to question him \n",
      "that a tanner in the city of Kiev was stronger than he \n",
      "a tanner in the city of Kiev \n",
      "this \n",
      "Nikita the Tanner \n",
      "him \n",
      "her \n",
      "this letter \n",
      "Nikita the Tanner \n",
      "his land \n",
      "the princess \n",
      "hides \n",
      "twelve hides \n",
      "that the tsar in person had come to see him \n",
      "the tsar in person \n",
      "him \n",
      "to tremble with fear \n",
      "he \n",
      "his hands \n",
      "the twelve hides \n",
      "him \n",
      "to go forth against the dragon \n",
      "five thousand little children \n",
      "them \n",
      "that their tears would move him to pity \n",
      "him \n",
      "The little children \n",
      "him \n",
      "fight the dragon \n",
      "the dragon \n",
      "to shed tears when he saw theirs \n",
      "tears \n",
      "theirs \n",
      "twelve thousand pounds of hemp \n",
      "it \n",
      "around himself \n",
      "him \n",
      "battle \n",
      "Nikita \n",
      "himself \n",
      "your lair \n",
      "to break down the door \n",
      "the door \n",
      "that he could not avoid trouble \n",
      "trouble \n",
      "him \n",
      "him \n",
      "to implore Nikita : `` Do not put me to death , Nikita the Tanner ; no one in the world is stronger than you and I \n",
      "me \n",
      "no one in the world \n",
      "us divide all the earth , all the world , into equal parts \n",
      "all the earth , all the world \n",
      "us draw a boundary line \n",
      "a boundary line \n",
      "a plow that weighed twelve thousand pounds \n",
      "a plow \n",
      "the dragon \n",
      "to plow a boundary from Kiev \n",
      "a boundary \n",
      "a furrow \n",
      "the whole earth \n",
      "the earth \n",
      "We have divided the earth \n",
      "us divide the sea \n",
      "the sea \n",
      "that your water has been taken \n",
      "your water \n",
      "him \n",
      "him \n",
      "That furrow \n",
      "it \n",
      "the fields \n",
      "the furrow \n",
      "what it is \n",
      "it \n",
      "it \n",
      "his heroic deed \n",
      "any reward \n",
      "Nikita \n",
      "hides \n"
     ]
    }
   ],
   "source": [
    "for pos in arg1s:\n",
    "    for i in range(pos[0], pos[-1] + 1):\n",
    "        print(document['tokens'][i] + ' ', end='')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 6],\n",
       " [22, 22],\n",
       " [33, 36],\n",
       " [43, 43],\n",
       " [43, 43],\n",
       " [43, 43],\n",
       " [66, 66],\n",
       " [73, 73],\n",
       " [77, 77],\n",
       " [77, 77],\n",
       " [84, 85],\n",
       " [89, 90],\n",
       " [92, 94],\n",
       " [105, 106],\n",
       " [115, 115],\n",
       " [127, 128],\n",
       " [127, 128],\n",
       " [143, 146],\n",
       " [143, 146],\n",
       " [153, 153],\n",
       " [167, 168],\n",
       " [167, 168],\n",
       " [184, 184],\n",
       " [192, 192],\n",
       " [209, 210],\n",
       " [214, 214],\n",
       " [214, 214],\n",
       " [214, 214],\n",
       " [228, 228],\n",
       " [240, 241],\n",
       " [240, 241],\n",
       " [240, 241],\n",
       " [247, 249],\n",
       " [247, 249],\n",
       " [266, 266],\n",
       " [266, 266],\n",
       " [279, 279],\n",
       " [282, 285],\n",
       " [304, 304],\n",
       " [315, 318],\n",
       " [322, 322],\n",
       " [322, 322],\n",
       " [332, 332],\n",
       " [332, 332],\n",
       " [341, 341],\n",
       " [332, 332],\n",
       " [348, 349],\n",
       " [356, 358],\n",
       " [364, 364],\n",
       " [364, 364],\n",
       " [373, 374],\n",
       " [373, 374],\n",
       " [380, 380],\n",
       " [384, 384],\n",
       " [384, 384],\n",
       " [384, 384],\n",
       " [404, 405],\n",
       " [384, 384],\n",
       " [384, 384],\n",
       " [427, 428],\n",
       " [444, 444],\n",
       " [448, 448],\n",
       " [459, 459],\n",
       " [459, 459],\n",
       " [467, 468],\n",
       " [472, 472],\n",
       " [467, 468],\n",
       " [467, 468],\n",
       " [487, 487],\n",
       " [503, 503],\n",
       " [508, 509],\n",
       " [508, 509],\n",
       " [540, 540],\n",
       " [554, 554],\n",
       " [573, 573],\n",
       " [577, 577],\n",
       " [584, 584],\n",
       " [584, 584],\n",
       " [601, 602],\n",
       " [601, 602],\n",
       " [611, 611],\n",
       " [627, 628],\n",
       " [631, 631],\n",
       " [640, 640],\n",
       " [648, 648],\n",
       " [653, 653],\n",
       " [659, 659],\n",
       " [670, 671],\n",
       " [680, 680],\n",
       " [680, 680],\n",
       " [720, 720],\n",
       " [720, 727],\n",
       " [734, 734],\n",
       " [734, 734],\n",
       " [734, 734]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('character-allennlp-4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f08a002cfb74b2506f54e27c15b4b151c42a9fcd7b5ae0eaf4159402e8079c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
