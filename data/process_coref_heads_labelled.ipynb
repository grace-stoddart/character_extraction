{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this notebook to convert original files with labelled coref info from JAhan, to dictionary format, compatible with feature construction using modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from misc import open_dict, save_dict, get_file_names, get_raw_text, get_raw_text_latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'CEN' \n",
    "# get_raw_text = get_raw_text_latin\n",
    "\n",
    "dataset = 'ProppLearner'\n",
    "\n",
    "labelledChainsDir = dataset + '/coref_heads_labelled/'\n",
    "formattedDir = dataset + '/coref_heads_labelled_new_format/'\n",
    "\n",
    "featuresDir = '../intermediate/' + dataset + '/from_original_jahan_heads_only/'\n",
    "animacyLabelsDir = featuresDir + 'animacy_labels_gold/'\n",
    "characterLabelsDir = featuresDir + 'character_labels_gold/'\n",
    "\n",
    "fileNames = get_file_names('data/' + labelledChainsDir, '.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileName in fileNames:\n",
    "    # with open(labelledChainsDir + fileName + '.txt') as f:\n",
    "    #     labChains = f.read()\n",
    "\n",
    "    labChains = get_raw_text(labelledChainsDir + fileName + '.txt')\n",
    "\n",
    "    chainsList = labChains.split(\"\\n\")\n",
    "    numChains = len(chainsList) - 1\n",
    "\n",
    "    characterLabels = np.zeros(numChains)\n",
    "    animateLabels = np.zeros(numChains)\n",
    "\n",
    "    corefDict = {'clusters':[], 'tokenizedDocument':''}\n",
    "\n",
    "    for i in range(len(chainsList)-1, -1, -1):\n",
    "        if chainsList[i] == '':\n",
    "            del chainsList[i]\n",
    "\n",
    "\n",
    "    for i, chain in enumerate(chainsList):\n",
    "\n",
    "            corefDict['clusters'].append({'name': '', 'mentions': []})\n",
    "\n",
    "            \n",
    "            splitChain = chain.split(\"|\")\n",
    "\n",
    "            characterLabels[i] = splitChain[0].split(\"\\t\")[1]\n",
    "            animateLabels[i] = splitChain[0].split(\"\\t\")[0]\n",
    "\n",
    "            for k in range(1, len(splitChain)-1):\n",
    "                ref = splitChain[k].strip()\n",
    "\n",
    "                corefDict['clusters'][i]['mentions'].append({'text': ref, 'positions':[]})\n",
    "\n",
    "    if dataset == 'ProppLearner':\n",
    "        fileName = fileName.lower()\n",
    "\n",
    "    save_dict(corefDict, formattedDir + fileName + '.p')\n",
    "\n",
    "    np.save(characterLabelsDir + fileName, characterLabels)\n",
    "    np.save(animacyLabelsDir + fileName, animateLabels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('character-allennlp-4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f08a002cfb74b2506f54e27c15b4b151c42a9fcd7b5ae0eaf4159402e8079c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
