{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This notebook takes original ProppLearner annotation files (StoryWorkbench format) and converts them into the dictionary structure used by my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import xmltodict \n",
    "from misc import save_dict, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawAnnFileLocation = \"ProppLearner/corefs_gold_xml/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'new_data/coref/story1.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_298605/4104643463.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# get basic token dict for allennlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpFileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"story\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstoryNum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".p\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mcorefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_data/coref/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpFileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtokenDictA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_MSc/character_identification_working/src/misc.py\u001b[0m in \u001b[0;36mopen_dict\u001b[0;34m(filePath)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'new_data/coref/story1.p'"
     ]
    }
   ],
   "source": [
    "### For each story, go through Story Workbench tokens vs AllenNLP tokens, and create a dictionary mapping one type to the other\n",
    "\n",
    "for storyNum in range(1, 16):\n",
    "\n",
    "    # get basic token dict for story workbench\n",
    "    fileName = \"story\" + str(storyNum) + \".sty\"\n",
    "\n",
    "    with open(rawAnnFileLocation + fileName, 'r', encoding='utf-8') as file:\n",
    "        xml = file.read()\n",
    "\n",
    "    dic = xmltodict.parse(xml)\n",
    "\n",
    "    tokenDictSW = {}\n",
    "    for i in range(len(dic['story']['rep'])):\n",
    "        if dic['story']['rep'][i]['@id'] == \"edu.mit.parsing.token\":\n",
    "            indexTokens = i\n",
    "            break\n",
    "\n",
    "    for token in dic['story']['rep'][indexTokens]['desc']:\n",
    "        tokenDictSW[int(token['@id'])] = {'word':token['#text']}\n",
    "\n",
    "\n",
    "    # get basic token dict for allennlp\n",
    "    pFileName = \"story\" + str(storyNum) + \".p\"\n",
    "    corefs = open_dict(\"new_data/coref/\" + pFileName)\n",
    "\n",
    "    tokenDictA = {}\n",
    "    for i, word in enumerate(corefs['tokenizedDocument']):\n",
    "        tokenDictA[i] = {'word':word}\n",
    "\n",
    "    # add allenNLP token indexes to the SW token dict\n",
    "    keysA = list(tokenDictA.keys())\n",
    "    keysSW = list(tokenDictSW.keys())\n",
    "\n",
    "    repeat = True\n",
    "    while repeat == True:\n",
    "        repeat = False\n",
    "\n",
    "        for index, (keyA,keySW) in enumerate(zip(keysA, keysSW)):\n",
    "            \n",
    "            # exact match\n",
    "            if tokenDictA[keyA]['word'] == tokenDictSW[keySW]['word']:\n",
    "                tokenDictSW[keySW]['Allen Index'] = keyA\n",
    "                continue\n",
    "\n",
    "            # difference in tokenization e.g. \" vs ''\n",
    "            lenKeysA = len(keysA)\n",
    "            lenKeysSW = len(keysSW)\n",
    "\n",
    "            # difference in tokenization e.g. \" vs ''\n",
    "            if index < lenKeysA - 1 and index < lenKeysSW - 1:\n",
    "                if tokenDictSW[keysSW[index + 1]]['word'] == tokenDictA[keysA[index + 1]]['word']:\n",
    "                    tokenDictSW[keySW]['Allen Index'] = keyA\n",
    "                    keysA = keysA[index+1:]\n",
    "                    keysSW = keysSW[index+1:]\n",
    "                    repeat = True\n",
    "                    \n",
    "            if repeat == True:\n",
    "                break\n",
    "\n",
    "            # allenNLP has grouped stuff\n",
    "            skip = 1\n",
    "            repeatSkipLoop = True\n",
    "            while skip < 6 and repeatSkipLoop:\n",
    "                if index < lenKeysSW - skip - 1:\n",
    "                    if tokenDictA[keysA[index + 1]]['word'] == tokenDictSW[keysSW[index + 1 + skip]]['word']:\n",
    "                        \n",
    "                        for i in range(skip + 1):\n",
    "                            tokenDictSW[keysSW[index+i]]['Notes'] = 'AllenNLP has grouped tokens'\n",
    "                            tokenDictSW[keysSW[index+i]]['Allen Index'] = keyA\n",
    "\n",
    "                        keysA = keysA[index+1:]\n",
    "                        keysSW = keysSW[index+skip+1:]\n",
    "                        repeat = True\n",
    "                        break\n",
    "                    else:\n",
    "                        skip += 1\n",
    "\n",
    "                else:\n",
    "                    repeatSkipLoop = False\n",
    "                    break\n",
    "\n",
    "            if repeat == True:\n",
    "                break\n",
    "            \n",
    "            # Story workbench has grouped stuff\n",
    "            skip = 1\n",
    "            repeatSkipLoop = True\n",
    "            while skip < 6 and repeatSkipLoop:\n",
    "                if index < lenKeysA - skip - 1 and index < lenKeysSW - 1:\n",
    "                    if tokenDictSW[keysSW[index + 1]]['word'] == tokenDictA[keysA[index + 1 + skip]]['word']:\n",
    "\n",
    "                        tokenDictSW[keySW]['Allen Index'] = []\n",
    "                        tokenDictSW[keySW]['Notes'] = \"Story Workbench has grouped tokens\"\n",
    "\n",
    "                        for i in range(skip+1):\n",
    "                            tokenDictSW[keySW]['Allen Index'].append(keysA[index + i])\n",
    "\n",
    "                        keysSW = keysSW[index+1:]\n",
    "                        keysA = keysA[index+skip+1:]\n",
    "                        repeat = True\n",
    "                        break\n",
    "                    else:\n",
    "                        skip += 1\n",
    "\n",
    "                else:\n",
    "                    repeatSkipLoop = False\n",
    "                    print('Check end of dict manually. Story num:',storyNum)\n",
    "                    break\n",
    "\n",
    "            if repeat == True:\n",
    "                break\n",
    "\n",
    "\n",
    "    # save story workbench token dict:\n",
    "\n",
    "    # save_dict(tokenDictSW, \"ProppLearner/storyworkbench_allen_tokens_map/\" + pFileName)\n",
    "    # print(\"story\",storyNum,\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now, use the dictionary created above to cinvert the StoryWorkbench coreference chains to my format, with AllenNLP indexes.\n",
    "\n",
    "for storyNum in range(1,16):\n",
    "\n",
    "    # get annotated dic file for story workbench\n",
    "    fileName = \"story\" + str(storyNum) + \".sty\"\n",
    "\n",
    "    with open(rawAnnFileLocation + fileName, 'r', encoding='utf-8') as file:\n",
    "        xml = file.read()\n",
    "\n",
    "    dic = xmltodict.parse(xml)\n",
    "\n",
    "    # get story workbench keys dict\n",
    "    pFileName = \"story\" + str(storyNum) + \".p\"\n",
    "    tokenDictStoryWorkbench = open_dict(\"old_with_indexes_data/storyWorkbench_tokenDict/\" + pFileName)\n",
    "\n",
    "    # get allenNLP coref dict (for tokenized document field only)\n",
    "    corefsA = open_dict(\"new_data/coref/\" + pFileName)\n",
    "\n",
    "    corefDict = {'clusters':[], 'tokenizedDocument':corefsA['tokenizedDocument']}\n",
    "\n",
    "\n",
    "    # get coref chains from story WB annotations\n",
    "    for i in range(len(dic['story']['rep'])):\n",
    "        if dic['story']['rep'][i]['@id'] == \"edu.mit.parsing.token\":\n",
    "            indexTokens = i\n",
    "        elif dic['story']['rep'][i]['@id'] == \"edu.mit.discourse.rep.coref\":\n",
    "            indexCoref = i\n",
    "        elif dic['story']['rep'][i]['@id'] == \"edu.mit.discourse.rep.refexp\":\n",
    "            indexRef = i\n",
    "\n",
    "    keys = list(tokenDictStoryWorkbench.keys())\n",
    "\n",
    "    for chainNum, chain in enumerate(dic['story']['rep'][indexCoref]['desc']):\n",
    "        title = chain['#text'].split('|')[0]\n",
    "        refs = chain['#text'].split('|')[1]\n",
    "\n",
    "        corefDict['clusters'].append({'name': title})\n",
    "\n",
    "        refs = refs.split(',')\n",
    "        \n",
    "        corefDict['clusters'][chainNum]['mentions'] = []\n",
    "\n",
    "        for k, ref in enumerate(refs):\n",
    "\n",
    "            # get position info from story workbench annotation file & allennlp Indexes\n",
    "            for refExppression in dic['story']['rep'][indexRef]['desc']:\n",
    "                \n",
    "                if int(ref) == int(refExppression['@id']):\n",
    "                    wordRefs = refExppression['#text']\n",
    "                    break \n",
    "\n",
    "            if ',' in wordRefs:\n",
    "                wordRefs = wordRefs.replace(\",\",\"~\")\n",
    "            \n",
    "            wordRefsList = wordRefs.split('~')\n",
    "            wordRefsList = [int(wordRefsList[0]), int(wordRefsList[-1])]\n",
    "\n",
    "            wordRefsAllenNLP = [tokenDictStoryWorkbench[wordRefsList[0]]['Allen Index'] ,tokenDictStoryWorkbench[wordRefsList[1]]['Allen Index']  ]\n",
    "\n",
    "            # get allenNLP indexes\n",
    "\n",
    "\n",
    "\n",
    "            # get corresponding text from storybook workbench dictionary\n",
    "            text = \"\"\n",
    "\n",
    "            startIndex = keys.index(wordRefsList[0])\n",
    "            endIndex = keys.index(wordRefsList[1])\n",
    "            for i in range(endIndex - startIndex + 1):\n",
    "\n",
    "                word = tokenDictStoryWorkbench[keys[startIndex + i]]['word']\n",
    "                text += word + ' '\n",
    "            \n",
    "            text = text.strip()\n",
    "\n",
    "            # add both of the above to dictionary, and append to list of mentions.\n",
    "            corefDict['clusters'][chainNum]['mentions'].append({'position': wordRefsAllenNLP, 'position_StoryWorkbench':wordRefsList, 'text':text})\n",
    "\n",
    "            # save the coref dict\n",
    "            save_dict(corefDict, \"ProppLearner/corefs_allen/\" + pFileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('character-allennlp-4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f08a002cfb74b2506f54e27c15b4b151c42a9fcd7b5ae0eaf4159402e8079c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
